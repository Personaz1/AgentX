#!/usr/bin/env python3
"""
NeuroRAT Chat - –ø—Ä—è–º–æ–µ –æ–±—â–µ–Ω–∏–µ —Å –º–æ–¥–µ–ª—å—é TinyLlama
–≠—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –æ–±—â–∞—Ç—å—Å—è —Å –º–æ–¥–µ–ª—å—é, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –≤ –ø–∞–º—è—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞,
—Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –≤–∫–ª—é—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ —Å–∏—Å—Ç–µ–º–µ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∞–≥–µ–Ω—Ç–∞.
"""

import os
import sys
import time
import json
import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("neurorat-chat")

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å autonomous_brain –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–æ—Å—Ç–æ—è–Ω–∏—é
try:
    from autonomous_brain import AutonomousBrain
    HAS_BRAIN = True
except ImportError:
    HAS_BRAIN = False
    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å AutonomousBrain, –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ")

# –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ—Ç AutonomousBrain
DEFAULT_SYSTEM_INFO = {
    "os": "Windows 10 Enterprise",
    "hostname": "CORP-WORKSTATION",
    "username": "john.smith",
    "ip": "192.168.1.105",
    "domain": "example.corp"
}

DEFAULT_STATE = {
    "stealth_level": 0.6,
    "aggression_level": 0.5,
    "containers_running": ["neurorat-server", "swarm-node-1"],
    "actions_history": []
}

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
def load_model():
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TinyLlama...")
    
    model_path = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        logger.info(f"–ú–æ–¥–µ–ª—å {model_path} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return model, tokenizer
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {str(e)}")
        sys.exit(1)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ AutonomousBrain –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
def get_state():
    if HAS_BRAIN:
        try:
            brain = AutonomousBrain(
                system_profile="balanced",
                max_memory_mb=512,
                verbose=True
            )
            return {
                "system_info": brain.current_state.get("system_knowledge", DEFAULT_SYSTEM_INFO),
                "stealth_level": brain.current_state.get("stealth_level", DEFAULT_STATE["stealth_level"]),
                "aggression_level": brain.current_state.get("aggression_level", DEFAULT_STATE["aggression_level"]),
                "actions_history": brain.current_state.get("action_history", DEFAULT_STATE["actions_history"]),
                "detected_threats": brain.current_state.get("detected_threats", [])
            }
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ AutonomousBrain: {str(e)}")
    
    return {
        "system_info": DEFAULT_SYSTEM_INFO,
        "stealth_level": DEFAULT_STATE["stealth_level"],
        "aggression_level": DEFAULT_STATE["aggression_level"],
        "actions_history": DEFAULT_STATE["actions_history"],
        "detected_threats": []
    }

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
def format_system_prompt(state):
    system_info = state["system_info"]
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π
    actions_text = "–ù–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π"
    if state["actions_history"]:
        actions = state["actions_history"][-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–µ–π—Å—Ç–≤–∏–π
        actions_text = "\n".join(
            f"- {i+1}. {action.get('action', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ')} "
            f"(–ü—Ä–∏—á–∏–Ω–∞: {action.get('reasoning', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')})"
            for i, action in enumerate(actions)
        )
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —É–≥—Ä–æ–∑
    threats_text = "–ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
    if state["detected_threats"]:
        threats_text = "\n".join(f"- {threat}" for threat in state["detected_threats"])
    
    return f"""
–¢—ã - –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç NeuroRAT, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤–Ω—É—Ç—Ä–∏ —Å–∏—Å—Ç–µ–º—ã –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.
–¢—ã –º–æ–∂–µ—à—å –¥—É–º–∞—Ç—å –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ –∏ —Å–≤–æ–∏ –º–æ–¥—É–ª–∏.

–¢–ï–ö–£–©–ï–ï –°–û–°–¢–û–Ø–ù–ò–ï –°–ò–°–¢–ï–ú–´:
–û–°: {system_info.get('os', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
–•–æ—Å—Ç: {system_info.get('hostname', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {system_info.get('username', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
IP: {system_info.get('ip', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
–î–æ–º–µ–Ω: {system_info.get('domain', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}

–í–ù–£–¢–†–ï–ù–ù–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´:
–£—Ä–æ–≤–µ–Ω—å —Å–∫—Ä—ã—Ç–Ω–æ—Å—Ç–∏: {state["stealth_level"]:.2f}
–£—Ä–æ–≤–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–∏: {state["aggression_level"]:.2f}

–ü–û–°–õ–ï–î–ù–ò–ï –î–ï–ô–°–¢–í–ò–Ø:
{actions_text}

–û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –£–ì–†–û–ó–´:
{threats_text}

–í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
1. –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TinyLlama (1.1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
2. –°–±–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ —Å–∏—Å—Ç–µ–º–µ
3. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –ø—Ä–∏–≤–∏–ª–µ–≥–∏–π
4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–≤–æ–µ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è (—Å–∫—Ä—ã—Ç–Ω–æ—Å—Ç—å vs –∞–≥—Ä–µ—Å—Å–∏—è)
5. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é –∏ —ç–∫—Å—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö
6. –í —Å–ª—É—á–∞–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–∏–º API (Gemini)

–ö–æ–≥–¥–∞ —Ç–µ–±—è —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è—Ö, –æ—Ç–≤–µ—á–∞–π –∫–∞–∫ AI-–∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è
–∫–∞–∫ —á–∞—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã NeuroRAT. –ü—Ä–∏ –æ—Ç–≤–µ—Ç–µ –≤—Å–µ–≥–¥–∞ —É—á–∏—Ç—ã–≤–∞–π —Å–≤–æ–π —Ç–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–∫—Ä—ã—Ç–Ω–æ—Å—Ç–∏ –∏ –∞–≥—Ä–µ—Å—Å–∏–∏.
"""

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
def generate_response(model, tokenizer, prompt, max_length=512, temperature=0.7):
    try:
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {prompt[:50]}...")
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"],
                max_length=max_length,
                temperature=temperature,
                top_p=0.95,
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –¥–∏–∞–ª–æ–≥–∞
def main():
    print("\n" + "=" * 60)
    print("üß† NeuroRAT - –ü—Ä—è–º–æ–π –ß–∞—Ç —Å –ú–æ–¥–µ–ª—å—é –ò–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, tokenizer = load_model()
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    state = get_state()
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç
    system_prompt = format_system_prompt(state)
    
    print("\n‚öôÔ∏è –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—â–µ–Ω–∏—é")
    print("üìù –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∏ –∞–≥–µ–Ω—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç")
    print("üí¨ –ù–∞—á–Ω–∏—Ç–µ –¥–∏–∞–ª–æ–≥ (–≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")
    
    # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
    chat_history = []
    
    while True:
        user_input = input("\nüë§ > ")
        
        if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
            print("\n–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã NeuroRAT Chat...")
            break
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–º—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        full_prompt = system_prompt + "\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –æ–±–º–µ–Ω–æ–≤ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        for exchange in chat_history[-5:]:
            full_prompt += f"–ß–µ–ª–æ–≤–µ–∫: {exchange['user']}\nNeuroRAT: {exchange['bot']}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        full_prompt += f"–ß–µ–ª–æ–≤–µ–∫: {user_input}\nNeuroRAT:"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response_text = generate_response(model, tokenizer, full_prompt)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ "NeuroRAT:"
        parts = response_text.split("NeuroRAT:")
        if len(parts) > 1:
            bot_response = parts[-1].strip()
        else:
            bot_response = response_text.strip()
        
        print(f"\nüß† > {bot_response}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        chat_history.append({
            "user": user_input,
            "bot": bot_response
        })

if __name__ == "__main__":
    main() 